{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING DE PAGE DYNAMIQUE : SELENIUM\n",
    "\n",
    "\n",
    "*OBJECTIF*: \n",
    "  - Utiliser le module **Selenium** de Python pour scraper les pages dynamiques\n",
    "\n",
    "Prérequis: \n",
    "  - Installer `selenium`\n",
    "  - Télécharger `driver` selon le navigateur de votre choix (l’objet qui gère le navigateur utilisé par Selenium)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BeautifulSoup** est une excellente bibliothèque pour récupérer des données sur le Web, mais elle ne traite pas du contenu créé de manière dynamique. Ce n’est en aucun cas une critique - Beautiful Soup fait exactement le travail qu’elle est censée faire et cela n’inclut pas le rendu de la page Web comme le ferait un navigateur.\n",
    "\n",
    "Afin d'obtenir ce contenu dynamique, la page Web doit être interprétée par un navigateur afin que le Javascript qui crée le contenu dynamique puisse faire son travail. Mais comment arriver au code HTML rendu par le navigateur? Une réponse consiste à utiliser un navigateur auxiliaire et la bibliothèque **Selenium** de Python. \n",
    "\n",
    "Nous allons utiliser un fichier HTML très simple qui contient du texte rendu dynamiquement. \n",
    "\n",
    "Copiez le code suivant dans un fichier nommé *test.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "   <title>Page Dynamique</title>\n",
    "</head>\n",
    "<body>\n",
    "   <div id=\"text\">Zone de texte</div>\n",
    "   <script>\n",
    "      document.getElementById(\"text\").innerHTML=\"Ceci est notre texte\"\n",
    "   </script>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout ce que nous avons ici est un fichier HTML avec un seul `<div>` dans le corps qui contient du texte («Zone de texte») mais lorsque la page est chargée, ce texte est remplacé par le texte généré par le Javascript plus bas. Le script localise le `<div>` par son id, `text`, puis définit le texte interne sur: «Ceci est notre texte»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant voyons ce qui se passe lorsque nous essayons de scraper cette page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Lecture du fichier\n",
    "soup_file=open(\"test.html\")\n",
    "# Parcourrir le code source\n",
    "soup = BeautifulSoup(soup_file)\n",
    "print(soup.title.get_text())\n",
    "print(soup.find(id='text').get_text())\n",
    "soup_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code ci-dessus, nous ouvrons le fichier html et par la suite nous utilisons BS pour parcourir le code source de la page.\n",
    "\n",
    "Mais comme vous pouvez le voir, nous n'avons récupéré que le contenu static de la page. Le texte généré par le JS n'as pu etre récupéré par BS. **C'est là que selenium rentre en jeu**\n",
    "\n",
    "## SELENIUM\n",
    "\n",
    "Selenium est un outil d’automatisation de test pour le web. Il permet de créer des **robots** qui naviguent dans des pages webs comme le ferait un vrai utilisateur. Bien que le premier rôle de Selenium soit le testing de pages webs (développement web), cet outil est beaucoup utilisé pour l’extraction de données.\n",
    "\n",
    "## Pourquoi utiliser Selenium pour scraper ?\n",
    "\n",
    "   La démarche habituelle pour scraper est : une requête suivie du parsing de la réponse. Dans la plupart des cas cela marche bien. Mais lorsque l’on a affaire à des sites avec plusieurs redirections ou avec des pop-ups avec lesquels il faut interagir avant d’avoir la page qu’on veut, cette démarche devient beaucoup moins amusante. Car il faut analyser le réseau et simuler les requêtes dans le bon ordre avec les bons arguments. Et parfois il est carrement impossible d’accéder à une page via une simple requête.\n",
    "\n",
    "   L’avantage de Selenium c’est que l’on (notre script) peut naviguer sur les pages. Du coup, si on voit la donnée dans notre navigateur, on peut la scraper via Selenium. Avec selenium, on peut remplir des formulaire, cliquer sur des boutons, scroller, parser des pages etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gecko_driver_path = \"put_path_to_your_driver.exe\"\n",
    "gecko_driver_path = \"../web_drivers/geckodriver.exe\"\n",
    "\n",
    "# Création d'une instance driver\n",
    "driver = webdriver.Firefox(executable_path=gecko_driver_path)\n",
    "\n",
    "# On va sur notre page test.html\n",
    "url = r\"file:///C:/Users/FJPN3406/Documents/FORMATIONS/WEB_SCRAPING/Web_Scraping_Dynamique/test.html\"\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étape suivante consiste à créer un objet BeautifulSoup et à y charger la source de la page. Nous pouvons ensuite extraire les données de cette source. Dans le code ci-dessous, vous pouvez voir que nous faisons à peu près la même chose que dans l'exercice précédent. Mais cette fois, le résultat sera différent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettez la source de la page dans une variable et créez un objet BS à partir de celle-ci \n",
    "soup_file = driver.page_source \n",
    "soup = BeautifulSoup (soup_file)\n",
    "# Charger et imprimer le titre et le texte de <div> \n",
    "print (soup.title.get_text ()) \n",
    "print(soup.find(id='text').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous pouvez le voir, nous utilisons maintenant le code qui a été traité par le navigateur Web driver, le résultat est ce qui serait rendu dans une fenêtre de navigateur, et non la source d'origine comme lors de notre première tentative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermez le navigateur \n",
    "chrome_driver.quit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple\n",
    "\n",
    "Nous voulons avoir les prix, titres d’offre et images pour toutes les offres concernants les IPhones sur le site de [http://www.ebay.fr](\"http://www.ebay.fr\"). Pour cela nous allons juste faire une recherche du mot clé “iphone” puis récupérer les informations. Simple non ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Le webdriver est l’objet qui gère le navigateur utilisé par Selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "# Pour utiliser toutes les touches du clavier\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Pour mettre le script en “pause” pour les chargement de page.\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gecko_driver_path = \"put_path_to_your_driver.exe\"\n",
    "gecko_driver_path = \"../web_drivers/geckodriver.exe\"\n",
    "\n",
    "# Création d'une instance driver\n",
    "driver = webdriver.Firefox(executable_path=gecko_driver_path)\n",
    "\n",
    "# Chargement de la page dans le driver\n",
    "url = \"https://www.ivoiremobiles.net\"\n",
    "driver.get(url)\n",
    "\n",
    "# En fonction de notre connexion et des performance de notre machine il faudra attendre\n",
    "# que la page charge avant de passer à la suite\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On recupère, grâce au selecteur de Selenium, l’élément qui correspond au champ “recherche”. Pour trouver le bon élément il faut inspecter la page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recupere la bar de recherche, on la remplit avec \"iphone\" puis on appuie \"Entrez\"\n",
    "search_bar = driver.find_element_by_name(\"search_query\")\n",
    "search_bar.send_keys(\"iphone\")\n",
    "search_bar.send_keys(Keys.ENTER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetez maintenant un oeil sur le webdriver généré par selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du code source de la page recherchée\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La liste des articles (Iphone)\n",
    "products = soup.find_all('div',class_='product-container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoir la balise contenant la description de l'article\n",
    "products[0].find(\"h5\",itemprop='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products[0].find(\"h5\",itemprop='name').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le prix de l'article\n",
    "products[0].find(\"span\",itemprop='price').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'image de l'article\n",
    "products[0].find(\"img\",itemprop='image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le lien de l'image\n",
    "products[0].find(\"img\",itemprop='image')['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le titre de l'image\n",
    "products[0].find(\"img\",itemprop='image')['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcourons maintenant la liste des articles pour avoir les infos sur tous les articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "names = []\n",
    "prices = []\n",
    "links = []\n",
    "\n",
    "for prd in products:\n",
    "        names.append(prd.find(\"h5\",itemprop='name').get_text())\n",
    "        prices.append(prd.find(\"span\",itemprop='price').get_text())\n",
    "        links.append(prd.find(\"img\",itemprop='image')[\"src\"])\n",
    "        \n",
    "df_iphones = pd.DataFrame({\"names\":names,\"prices\":prices,\"link_image\":links})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_iphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nous avons maintenant un dataframe contenant pour les téléphones Iphone disponible sue le site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
